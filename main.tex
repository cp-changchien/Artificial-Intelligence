\documentclass[a4paper,10pt]{article}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{subcaption}
\usepackage[latin1]{inputenc}
\usepackage[numbers]{natbib}
\usepackage{amsmath}
\bibliographystyle{unsrtnat}
\usepackage[top=1.5cm,bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{siunitx}
\usepackage[parfill]{parskip}
\usepackage{multirow}
\setlength{\parskip}{1em}


\begin{document}

\begin{center}
    {\textbf{\Large{A.I. notes}}}\\
    {\large{Cooper}}
\end{center}

%%%%%%%%%% Main content %%%%%%%%%%
\section{Linear Regression}
\begin{itemize}
    \item \textbf{Hypothesis} function
        \begin{gather*}
            y = h(x) = \theta_0+\theta_1x\\
            y = h(x) = \pmb{\theta}\cdot\pmb{x}\\
        \end{gather*}

        \vspace*{-0.5cm}
    \item \textbf{Loss function}
        \begin{gather*}
            J(\theta_0. \theta_1) = \frac{1}{2}\frac{1}{m}\sum_{i=1}^{m}\Big(h(x^{(i)})-y_i\Big)^2 = \frac{1}{2}\frac{1}{m}\sum_{i=1}^{m}\Big(\theta_0+\theta_1x^{(i)}-y_i\Big)^2\\
            J(\theta_0,\theta_1,\dots,\theta_N) = \frac{1}{2}\frac{1}{m}\sum_{i=1}^{m}\Big(\theta_0+\sum_{k=1}^{N}\theta_kx_k^{(i)}-y_i\Big)^2
        \end{gather*}
    \item \textbf{Calculate the Gradient} gradient is the direction along which the loss function increases the most
        \begin{gather*}
            \nabla_\theta J_{(0)} = 
            \begin{pmatrix}
                \frac{\partial J}{\partial\theta_0} \\[0.4cm]
                \frac{\partial J}{\partial\theta_1} \\
            \end{pmatrix}_{(0)}
            \hspace*{0.7cm}
            \nabla_\theta J_{(0)} = 
            \begin{pmatrix}
                \frac{\partial J}{\partial\theta_0} \\[0.2cm]
                \frac{\partial J}{\partial\theta_1} \\[0.2cm]
                \vdots \\ 
                \frac{\partial J}{\partial\theta_N} \\
            \end{pmatrix}_{(0)}
        \end{gather*}

        \vspace*{-0.4cm}
    \item \textbf{Update}
        \begin{gather*}
            \pmb{\theta}_{(1)} = \pmb{\theta}_{(0)} - \alpha\nabla_\theta J_{(0)}\\
            \pmb{\theta}_{(n+1)} = \pmb{\theta}_n - \alpha\frac{1}{m}\mathbf{X}^T(\mathbf{X}\pmb{\theta}_n-\pmb{y})
        \end{gather*}
    \item \textbf{Repeat}.
        \begin{gather*}
            \pmb{\theta}_{(n+1)} = \pmb{\theta}_{(n)} - \alpha\nabla_\theta J_{n}\hspace*{0.5cm}n = 0,1,2,\dots\\
            \text{Stop if}\hspace*{0.3cm}\left| J_{(n+1)}-J_n \right|\leq\text{tol}\hspace*{0.5cm}\text{or}\hspace*{0.3cm}n+1\geq\text{ max}
        \end{gather*}
\end{itemize}

\subsection{Regularisation}
A good machine learning model should have a good \textit{bias-variance trade-off}. This can be achieved by minimising the generalisation error, which is the sum of the squared bias and the variance. 
\begin{gather*}
    J(\theta_0. \theta_1) = \frac{1}{2}\frac{1}{m}\sum_{i=1}^{m}\Big(h(x^{(i)})-y_i\Big)^2 + \lambda\sum_{j=1}^{N}\theta_j^2
\end{gather*}
If $\lambda$ is small, large values of $\sum_{j=1}^{N}\theta_j^2$ are allowed, meaning that the model contains too many parameters, thus it can \textbf{overfit}. If $\lambda$ is large, only small values of $\sum_{j=1}^{N}\theta_j^2$ are allowed. This means that the training will reduce the number of parameters, this it might underfit. 

\section{Logistic Regression}
\begin{itemize}
    \item \textbf{Hypothesis function}, models the probability that a given set of features belongs
    to class
        \begin{gather*}
            h(\mathbf{x}) = \sigma(\mathbf{\theta\cdot x}) = \frac{1}{1+exp(-\theta\cdot x)}
        \end{gather*}
    \item \textbf{Loss function} (cross-entropy), entropy measures uncertainty on the possible values of a prediction
        \begin{gather*}
            J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}\Big\{y_i ln\Big[h(x^{(i)})\Big] + (1-y_i)ln\Big[1-h(x^{(i)})\Big]\Big\}
        \end{gather*}
    \item \textbf{Gradient descent} for logistic Regression:\par
    Choose a starting point $\Rightarrow$ Calculate hte gradient $\Rightarrow$ Update and repeat. 
    \item If encounter \textbf{multi-class}, perform the same as many times as the number of categories of logistic regressions, each of which splits the data into two classes. End up with the same number of Hypothesis functions, select the highest probability.
    \item Metrics for classification 
        \begin{table}[hbt]
            \centering
            \begin{tabular}{lcccl}
            \cline{3-4}
                                & \multicolumn{1}{l|}{}           & \multicolumn{2}{c|}{\textbf{Ground Truth}}                               &  \\ \cline{3-4}
                                & \multicolumn{1}{c|}{}           & \multicolumn{1}{c|}{\textbf{1}}     & \multicolumn{1}{c|}{\textbf{0}}    &  \\ \cline{1-4}
            \multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Model\\ Prediction\end{tabular}}}} &
            \multicolumn{1}{c|}{\textbf{1}} &
            \multicolumn{1}{c|}{True Positive} &
            \multicolumn{1}{c|}{False Positive} &
            \multicolumn{1}{c}{\textbf{Precision}} \\ \cline{2-4}
            \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\textbf{0}} & \multicolumn{1}{c|}{False Negative} & \multicolumn{1}{c|}{True Negative} &  \\ \cline{1-4}
                                & \multicolumn{1}{l}{}            & \textbf{Recall}                     & \multicolumn{1}{l}{}               & 
            \end{tabular}
        \end{table}
\end{itemize}

\subsection{Supporting Vector Machines (SVMs)}
In SVMs, we want to compute the optimal linear decision boundary, which is the \textbf{hyperplane} with the largest margin between the two classes.
\begin{itemize}
    \item Support hyperplanes (optimal hyperplane between two points)
    \begin{gather*}
        \mathbf{w}\cdot\mathbf{x} + b = 1\\
        \mathbf{w}\cdot\mathbf{x} + b = -1\\
    \end{gather*}

    \vspace*{-0.8cm}
    How is the hyperplane with largest margin oriented can be found by solving an optimisation problem
    \begin{gather*}
        \text{Find }\mathbf{w},b\text{ such that }\frac{2}{\lVert w \rVert}\text{ is maximum}\\
        \text{or }\min\limits_{(w,b)}\frac{1}{2}{\lVert w \rVert}^2\hspace*{0.3cm}\text{subject to}\hspace*{0.3cm}y_i(\mathbf{w}\cdot\mathbf{x}^{(i)}+b)\geq1\hspace*{0.3cm}i = 1,2,\dots,m
    \end{gather*}
\end{itemize}

\subsubsection{Rgularisation}
if the data is not linearly separable, we need to introduce regularisation to allow some misclassified data points by introducing a slack variable $\varepsilon_i>0$ such that: (i) $\varepsilon_i=0$ for correctly classified (ii) $\varepsilon_i=\left| y_i-f(\mathbf{x}^{(i)})\right|$ otherwise. 
\begin{gather*}
    \min\limits_{(w,b,\varepsilon)}\frac{1}{2}{\lVert w \rVert}^2+C\sum_{i}^{m}\varepsilon_i\hspace*{0.3cm}\text{subject to}\hspace*{0.3cm}y_i(\mathbf{w}\cdot\mathbf{x}^{(i)}+b)\geq1-\varepsilon_i\hspace*{0.3cm}\varepsilon_i\geq0\\
    \text{small C - soft (wide margin)}\hspace*{0.4cm}C\rightarrow\infty\text{ - hard margin}
\end{gather*}

\subsubsection{Kernel Trick}
For nonlinearly separable data, one option is to enrich the data with nonlinearities. This can be achieved by mapping the data into a nonlinear higher-dimensional space, $\mathbf{x}\rightarrow\mathbf{\phi(x)}$, where $\mathbf{\phi}$ is a nonlinear map, for example, a polynomial $(x_1^{(i)},x_2^{(i)})\rightarrow(x_1^{(i)},x_2^{(i)},x_1^{(i)2}+x_2^{(i)2})$. Intuitively, by adding dimensions in the feature space, it is more likely that the data becomes linearly separated.\par 

Kernel Trick decomposes the weight vector in the transformed feature space
\begin{gather*}
    \mathbf{w} = \sum_{j=1}^{m}\beta_j\pmb{\phi}(\pmb{x}^{(j)})
\end{gather*}
where $\beta_j$ become the new weights to find. In other words, we express the weight vector with coordinates defined in the transformed feature space.
\begin{gather*}
    f(\pmb{x}^{(i)}) = \sum_{j=1}^{m}\beta_j\pmb{\phi}(\pmb{x}^{(j)})\cdot\pmb{\phi}(\pmb{x}^{(i)})+b 
\end{gather*}
we resort to a theorem of functional analysis, which ensures that there exists a function, $K$, such that
\begin{gather*}
    K(\pmb{x}^{(j)}\cdot\pmb{x}^{(i)}) = \pmb{\phi}(\pmb{x}^{(j)})\cdot\pmb{\phi}(\pmb{x}^(i))
\end{gather*}
where $K$ is the \textit{kernel}. $K$ suggested that we DO NOT need to increase the dimension of the feature space to include nonlinearities, we only need to apply the kernel to inner products in the original feature space. essentially finding optimal separating hyperplane without calculating anything about $\pmb{\phi(x^{(i)})}$.





\section{Feedforward Neural Network}
\textit{Neural Network generalise the principle of linearly combining simple nonlinear functions}
\begin{gather*}
    y\approx h(x:\theta)
\end{gather*}
function that maps input data "x" to output "y", with the function parameters represented by "$\theta$".

\begin{figure}[ht]
    \centering
    \includegraphics[width =0.5\textwidth]{FNN.png}
\end{figure}

\subsection{Mathematical expression}
\begin{itemize}
    \item Input layer $\rightarrow$ First hidden layer
        \begin{gather*}
            x  = 
            \begin{pmatrix}
                x_{1} \\
                x_{2} \\
                \vdots \\
                x_{m} \\
            \end{pmatrix}
            \Rightarrow\hspace*{0.4cm}\theta^{(0)} = 
            \begin{pmatrix}
                \theta_{0,1}^{(0)} & \theta_{0,2}^{(0)}  \\
                \theta_{1,1}^{(0)} & \theta_{1,2}^{(0)}  \\
                \vdots \\
                \theta_{N,1}^{(0)} & \theta_{N,2}^{(0)}  \\
              \end{pmatrix}
              ,\hspace*{0.3cm}
              z^1 = 
              \begin{pmatrix}
                b_0^1 \\
                \theta^{(0)T}x \\
              \end{pmatrix}
              ,\hspace*{0.3cm}
              a^1 = a\{z^1\} 
        \end{gather*}
    where $z$ is the \textit{pre-activation}
    \item Second hidden layer 
        \begin{gather*}
        \theta^{(1)} =
            \begin{pmatrix}
                \theta_{0,1}^{(1)} & \theta_{0,2}^{(1)} & \theta_{0,3}^{(1)}  \\
                \theta_{1,1}^{(1)} & \theta_{1,2}^{(1)} & \theta_{1,3}^{(1)} \\
                \theta_{2,1}^{(1)} & \theta_{2,2}^{(1)} & \theta_{2,3}^{(1)} \\
            \end{pmatrix}
            ,\hspace*{0.3cm}
            z^2 = 
            \begin{pmatrix}
                b_0^2 \\
                \theta^{(2)T}a^1 \\
            \end{pmatrix}
            ,\hspace*{0.3cm}
            a^2 = a\{z^2\} 
        \end{gather*}
    \item Output layer 
        \begin{gather*}
            \theta^{(1)} =
                \begin{pmatrix}
                    \theta_{0,1}^{(2)} & \theta_{0,2}^{(2)} \\
                    \theta_{1,1}^{(2)} & \theta_{1,2}^{(2)} \\
                    \theta_{2,1}^{(2)} & \theta_{2,2}^{(2)} \\
                    \theta_{3,1}^{(2)} & \theta_{3,2}^{(2)} \\
                \end{pmatrix}
                ,\hspace*{0.3cm}
                z^3 = \theta^{(2)T}a^2
                ,\hspace*{0.3cm}
                h = a^3 = a\{z^3\} 
            \end{gather*}
            Or in generalised from
            \begin{gather*}
                a^{L+1} = a\{\theta^{(L)T}a^L\}
            \end{gather*}
\end{itemize}

\subsection{Number of parameters}
\begin{gather*}
    \begin{split}
        \text{n. of parameters} &= \underbrace{(s_0+1)\times s_1}_{(N+1)\times 2} + \underbrace{(s_1+1)\times s_2}_{3\times 3} + \underbrace{(s_2+1)\times s_3}_{4\times 2} = (N+1)\times 2+17\\
        & = \sum_{i=0}^{L}(s_i+1)\times s_{i+1}
    \end{split}
\end{gather*}

\subsection{Training}
Training dataset (60-8-\%), Validation (10-20\%), and test (10-20\%)\par 

Initialise $\Rightarrow$ Forward propagation $\Rightarrow$ Backpropagation $\Rightarrow$ Update $\Rightarrow$ Repeat until convergence



\section{Convolution Neural Network}
Drawbacks from feed-forward neural networks 
\begin{itemize}
    \item textit{fully connected} layers not suitable for images: contains too many parameters  
    \item need to \textit{flatten} 2D data into 1D array --- losing spatial information. 
\end{itemize}

Generic process
\begin{itemize}
    \item input $\rightarrow$ filter (convolutional layer)$\rightarrow$ pooling layer $\rightarrow$ (repeat) $\rightarrow$ flatten $\rightarrow$ fully-connected layer
    \item Numbers of neurons in a convolutional layer
        \begin{gather*}
            N_d = \frac{W_d-F_d+2P_d}{S_d}+1
        \end{gather*}
    where $N_d$ is the number of neurons, $W$ is the width of the input channel, $F$ is the width of the filter, $P$ is the width of the pooling, and $S$ is the Stride number. 
    \item Number of parameter in convolution layer $l$
        \begin{gather*}
            l = (F_x\times F_y\times \text{n. channels} + 1 \text{ bias})\times K
        \end{gather*}
    where $K$ is the number of filters (also =number of feature maps)
\end{itemize}



\section{Sequence Modelling}
When both the data and its order contain information to perform the task, the points in the dataset are dependent on the position of other points in the dataset, e.g., speech recognition, DNA, time series. We lose key information if we shuffle the data.

\subsection{Sequence modelling with feedforward neural network}
Consider time as \textbf{feature (inputs)}, and the physical variable as \textbf{label}, may be OK for short time series, but generally not a good approach due to:
\begin{itemize}
    \item do not learn correlation as opposed to dynamical system having temporal correlations, and patterns. 
    \item large number of parameters
    \item needs the inputs of a fixed size. 
\end{itemize}

\subsection{Recurrent cell}
Store the previous time-step and feed it to the next time step as the hidden state.
\vspace*{0.3cm}
\begin{gather*}
    a^{<N>} = a(\theta_a^{<N-1>}+\theta x^{<N>}+b)\\
    h^{<N>} = \theta_ha^{<N>}+b_h
\end{gather*}

\vspace*{-0.3cm}
\begin{figure}[ht]
    \centering
    \includegraphics[width =0.6\textwidth]{RC.png}
\end{figure}

\begin{itemize}
    \item The weights and biases are shared across the time-step
    \item \textbf{time is not a feature}
    \item the loss function is a sum of the loss functions of every time step by applying chain rule across different time steps. 
        \begin{gather*}
            J = \sum_{i=0}^{N}J^{<N>}\\
            \frac{\partial J}{\partial a^{<i>}} = \frac{\partial J}{\partial a^{<N>}}\frac{\partial a^{<N>}}{\partial a^{<N-1>}}\dots\frac{\partial a^{<i+1>}}{\partial a^{<i>}} = \frac{\partial J}{\partial a^{<N>}}\underbrace{\theta_a\theta_a\dots\theta_a}_{N-i\text{ times}} = \frac{\partial J}{\partial a^{<N>}}\theta_a^{N-1}
        \end{gather*}
    \item Unstable gradient occurs from above equation, when $\theta_a$ is small, the gradient will become smaller and leads to \textbf{vanishing gradient} problem,  make it difficult for the network to remember information from previous time steps. This can be solved by using ReLU activation functions due to its unbounded positive range. 
    \item If $\theta_a$ is large, this leads to exploding gradient, which can be solved by clip (rescale) the gradient
    \item Recurrent neural network are not ideal for learning temporal correlations in long time series due to short-term memory. 
\end{itemize}

\subsection{Long-short time memory units LSTMs}
Introducing a \textit{cell state C} to convey (store) information from the far past -- long term memory, with the combination of the same short-memory past. 
\begin{itemize}
    \item Consists of three gates to control the status of the states (forget, input, output). Each gate is controlled by a \textbf{sigmoid} activation function, due to the extreme nature of being either 1 or 0.
    \item \textbf{Forget gate.} This determines what percentage of the Long-term memory is remembered. 
    \item \textbf{Candidate state} combines the shor-term memory and the input to create a \textbf{potential long-term memory} $g$. By using \textbf{tanh} to restrain the data in the range of -1 to 1. 
    \item \textbf{Input gate} determines what percentage of the potential long-term memory is added to the long term memory $C$.
    \item \textbf{Output gate} calculates the potential long term memory from the cell state using a tanh function. Then, a sigmoid function is also used to determine what percentage of this potential long term memory is pass on as an output of the LSTM unit. 
\end{itemize}

\subsection{Gated recurrent units GRUs}
\begin{itemize}
    \item got rid of the cell state, and stored both long and short term memory in the \textbf{hidden state} and consists of 2 gates only.
    \item \textbf{Update gate} controls both the \textit{input} and \textit{forget} gate in determining what to forget and what to add on. 
    \item \textbf{Reset gate} decides how much past information to forget. 
    \item \textbf{Candidate state} contains past information.
    \item GRUs are simpler and with less parameters, making them easier to train than LSTMs
\end{itemize}

\vspace*{-0.3cm}
\begin{figure}[ht]
    \centering
    \includegraphics[width =0.7\textwidth]{LSTM_GRU.png}
\end{figure}


\section{Unsupervised Learning}
Methods that do not require labelled dataset. \textbf{Clustering} groups the instances based on their similarity without class labels.
\subsection{k-Means clustering (Lloyd-Forgy algorithm)}
Given a set of vector-valued data, $\mathbf{x}^{(i)}$, with the goal of grouping $m$ observation into $k$ clusters. k-means algorithm is iterative and is described as following:
\begin{itemize}
    \item \textbf{Choose a metric (distance function)}. This quantitatively defines the notion of \textit{similarity} by measuring the distance between points in the \textit{feature space}. The distance are usually calculated using the squared Euclidean distance $\lVert\cdot\rVert^2$.
    \item \textbf{Choose number of clusters, $k$}
    \item \textbf{Choose the centroids} These are the baricentres (means), $\mu_j$, of the clusters, representing the positions of the clusters. 
    \item \textbf{Compute the distance and Assign} of each instance with respect to each centroid, $\lVert\pmb{x}^{(i)}-\pmb{\mu}_j\rVert^2$ for $i=1,2,\dots, k$.
    \item \textbf{Compute the baricentre} of each centroid, $\mu_{j,new}=1/N_j\sum_{j=1}^{N_j}\pmb{x}^{(j)}$, where $N_j$ is the number of points, $\pmb{x}^{(i)}$, that belong in cluster j. Therefore, $\sum_{i=1}^{k}N_i=m$. 
    \item \textbf{Update centroid}. The new centroid is the baricentre from the previous new mean. 
    \item Compute the new distance, under the mean distance from the centroid stops changing. 
\end{itemize}

Although k-means algorithm presents a fast and effective method to cluster data points, it has some limits: (i) though the algorithm is guaranteed to converge, it might not be the globally optimal solution, mainly because it depends on the initial centroid initialisation and numbers of cluster chosen. (ii) it noes not perform well when clusters have varying sizes.\par 

There are several approaches to be taken to optimise the solution: (i) different centroid initialisation, (ii) number of clusters, a simple method is to plot the variance and the number of cluster, and choose the number of clusters at the inflection point (elbow). This is called the \textit{elbow method}. (iii) Feature scaling, scale the input features before running k-means. 

\subsection{Density-based spatial clustering of applications with noise (DBSCAN)}
A clustering algorithm that can handle nested clusters, and identifying nested algorithms at higher dimensions where we can't perceive by eye. DBSCAN groups points that are closely-packed, identifies the points that are isolated as outliers. 
\begin{itemize}
    \item \textbf{Choose a metric (distance function)} --- Euclidean distance, $\lVert\cdot\rVert^2$
    \item \textbf{Choose the vicinity}, $\varepsilon$ , to other points, i.e., the neighbourhood $\lVert\pmb{x}^{(i)}-\pmb{x}^{(j)}\rVert^2\leq\varepsilon$. Choose $N$ to define the number of points that are needed to classify a neighbourhood as high density. 
    \item \textbf{Select a instance} and \textbf{Count} how many instances are located with distance $\varepsilon$ from it. 
    \begin{itemize}
        \item If an instance has at least $N$ in its $\varepsilon$, then it is considered as a \textit{core instance}. All instances in $\varepsilon$ of a core instance belong to the same cluster. This may also include other core instances. 
        \item If fewer that $N$ belong in $\varepsilon$, it is identify as \textit{Non-core} instance. 
    \end{itemize}
    \item \textbf{Classify} anomalies that are not core instances. 
\end{itemize}

DBSCAN provides the advantage of not needing to specify the number of clusters, can find clusters with complicated shapes and can also identifies outliers. However, it does not cluster effectively when there are large differences in densities. 


\subsection{Hierarchical clustering}
Hierarchical clustering does not required user input cluster numbers, it does it by inspecting the result. This section will be focusing on using \textbf{Bottom-up}, in which each instance defines a cluster a iteration 0. The algorithm then paris and merge clusters. 
\begin{itemize}
    \item \textbf{Choose a metric (distance function)} --- Euclidean distance, $\lVert\cdot\rvert^2$.
    \item \textbf{Compute} the distance between all the instances. 
    \item \textbf{Merge} the closest pair into a new cluster, which is located midway its original instance. 
    \item \textbf{Repeat} Step 1-2 with $m-1$ instances. 
    \item \textbf{Stop} when left with one cluster.  
\end{itemize}

\subsection{Elliptic envelope for anomaly detection}
Using Gaussian density distribution as an approach to identify outliers. 
\subsubsection{Multivariate Gaussian distribution}
A column vector of random variable (feature), $\pmb{X} = [X_1, X_2, \dots, X_N]^T$ is Gaussian distributed. 
\begin{gather*}
    f_{\pmb{x}}(\pmb{x}) = \frac{1}{\sqrt{(2\pi)^N\text{det}(\Sigma)}}\text{exp}\Big(-\frac{\lVert\pmb{x-\mu}\rVert^2_\Sigma}{2}\Big)
\end{gather*}
where $\pmb{\mu}$ is the column-vector if the means, 
\begin{gather*}
    \pmb{\mu}=\frac{1}{m}\sum_{i=1}^{m}\pmb{x}^{(i)}
\end{gather*}
and $\Sigma$ is the \textbf{covariance matrix}, which can be computed from a sample, $m$, as (usually presented as matrix). Covariance matrix is \textbf{symmetric} and can give insights into whether the variable are correlated (only diagonal values, or the principal axes of the ellipsoid is not parallel to the feature axis), or uncorrelated (non-diagonal)
\begin{gather*}
    \pmb{\Sigma} = \frac{1}{m-1}\sum_{i=1}^{m}(\pmb{x}^{(i)}-\pmb{\mu})(\pmb{x}^{(i)}-\pmb{\mu})^T
\end{gather*}
And finally the norm that measures the distance between the mean and a feature is
\begin{gather*}
    \lVert\pmb{x}-\pmb{\mu}\rvert^2_\Sigma = (\pmb{x}-\pmb{\mu})^T\pmb{\Sigma}^{-1}(\pmb{x}-\pmb{\mu})
\end{gather*}
The inverse of the covariance matrix, $\Sigma^{-1}$, is also known as the \textit{precision} matrix. 
\subsubsection{Elliptic envelope algorithm}
elliptic envelope algorithm is a distance-based anomaly detection method that can be summarised as 
\begin{enumerate}
    \item \textbf{Assume} the datapoints to be Gaussian distributed
    \item \textbf{Compute} mean and covariance (covariance matrix also defines the shape of the ellipse)
    \item \textbf{Compute} the distance, $D$, between testpoints and the mean
    \item \textbf{Choose} the decision boundary, $c$ (the range of the elliptic)
    \item \textbf{If} $D>c$, the test point is an outlier (anomaly). 
\end{enumerate}


\section{Decision Tree}
Decision tree contains three main components: (i) \textbf{Internal nodes} that provides classification criteria, (ii) \textbf{Branches} that listed out the features that satisfies the previous criteria, and (iii) \textbf{End nodes (leaves)} that is the final result for classification. \par

Noteworthy that when classifying between binary classes, the criteria can be set for e.g., $x_1=3$ where equal sign could be used. But when dealing with numerical values, \textbf{inequalities} must be used, $x\leq3$.

\subsection{Impurity Measures}
Decision tree is constructed in the learning process node by node. The following nodes, or \textit{children nodes}, for the partition are constructed with minimum impurity. Two impurity measure will be discussed here --- \textit{information gain, Gini diversity index}
\begin{itemize}
    \item Information gain
        \begin{itemize}
            \item information gain tries to find a suitable separation line in the features that can classify data in a more effective way. e.g. by shuffling the order of the feature:
            \begin{table}[hbt]
                \centering
                \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
                \hline
                $x_1$ & 3 & 7 & 3 & 4 & 3 & 6 & 5 & 6 \\ \hline
                $x_2$ & 5 & 6 & 3 & 8 & 9 & 5 & 8 & 4 \\ \hline
                label & pass & fail & pass & fail & fail & pass & fail & fail \\ \hline
                \end{tabular}
            \end{table}
            \vspace*{-0.3cm}
            \begin{table}[hbt]
                \centering
                \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
                \hline
                $x_1$ & 3 & 6 & 3 & 6 & 7 & 4 & 5 & 3 \\ \hline
                $x_2$ & 3 & 4 & 5 & 5 & 6 & 8 & 8 & 9 \\ \hline
                label & pass & fail & pass & pass & fail & fail & fail & fail \\ \hline
                \end{tabular}
            \end{table}
            In this case, we can place a separation line at $x_2\geq6$ to better separate the fail by one node. 
            \item \textbf{Entropy} can be used to evaluate the difficulty of predicting category.
            \begin{gather*}
                H = -\sum_{i = 1}^{n}(p_i log_2p_i)
            \end{gather*}
            The aim is to find features with \textbf{highest information gain} (this feature will be split into children nodes to further). \textbf{IG} is \textbf{highest} when splitting the feature makes the biggest reduction in entropy.
            \begin{gather*}
                IG(feature) = H(\text{label}) - H(\text{label}, feature)
            \end{gather*}
        \end{itemize}
    \item Gini Impurity Criterion
        \begin{itemize}
            \item How often a randomly chosen element from the set is incorrectly labelled. Higher GI, higher chance of misclassification
            \begin{gather*}
                GI = 1-\sum_{i=0}^{J}P_i^2
            \end{gather*}
            \item Split with the lowest weighted FI for the child trees to achieve largest reduction in GI.
        \end{itemize}
    \item Shallow trees has less variance but higher bias. Deep trees has less bias but more prone to overfitting. 
\end{itemize}


\section{Principal Component Analysis (PCA)}


%%%%%%%%%% %%%%%%%%%%



\end{document}
 